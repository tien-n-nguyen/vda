\subsection{Experimental Methodology}
\label{method:sec}

%

%To evaluate the performance of automated commit-level vulnerability assessment, we use the following two metrics: F1-Score and Matthews Correlation Coefficient (MCC). These two metrics have been widely, commonly, and frequently in the literature (check deepcva page 6, evaluation metrics for literature). Due to the fact that our {\tool} is built for multi-class classification, we used the macro F1-Score and multi-class version of MCC as in~\cite{le2021deepcva}.


%\subsubsection{\textbf{Experimental Methodology.\\}}
%\vspace{-6pt}

%\noindent\textbf{RQ1. Comparison on Vulnerability Detection on C dataset}

\subsubsection{Comparison on Vulnerability Detection on BigVul (RQ1).~\\}

\emph{Baselines}. We include {\bf VCCFinder}~\cite{perl2015vccfinder}
(a commit-level ML-based VD), and other ML VD tools:
\textbf{VulDeePecker} \cite{li2018vuldeepecker}, \textbf{Devign}
\cite{zhou2019devign}, \textbf{SySeVR} \cite{li2021sysevr},
\textbf{Russell} {\em et al.}  \cite{russell2018automated},
\textbf{Reveal} \cite{chakraborty2021deep}, and {\bf IVDetect}
\cite{li2021vulnerability}. Except VCCFinder, we ran the others
on the code after commit. 

\emph{Procedure.}  We used all vulnerable methods and randomly selected
the same number of non-vulnerable methods from the fixed version
projects, to build a dataset with the vul:non-vul ratio of 1:1. We
also evaluated the tools with the real-world ratio of 9:1. We randomly split the
data 80\%, 10\%, 10\% on the project basis without changing the
vul:non-vul ratio for training, tuning, and testing.

\emph{Parameter Tuning.}  For {\tool}, we used
autoML~\cite{NNI} for tuning the following hyper-parameters to have the
best performance: (1) Epoch size (100, 200, 300); (2) Batch size (64,
128, 256); (3) Learning rate (0.001, 0.003, 0.005, 0.010); (4) word embeddings length (150, 200, 250, 300). We
tuned DeepCVA's parameters from its documentation.
%We use AutoML~\cite{NNI} on all models to automatically tune
%hyper-parameters on the tuning dataset. We tuned \tool with the
%parameters \textit{batch size, hidden size, learning rate, dropout
%  rate}. The hyper-parameters we tuned for the baselines can be found
%in their papers. We choose the hyper-parameters with the best accuracy
%for each model.

\emph{Evaluation Metrics.} We use the following metrics to measure the
effectiveness of a model: $Recall = \frac{TP}{TP+FN}$, $Precision =
\frac{TP}{TP+FP}$, and $F$-$score =
\frac{2*Recall*Precision}{Recall+Precision}$.
%\begin{equation}
%	Recall = \frac{TP}{TP+FN}
%\end{equation}
%\begin{equation}
%	Precision = \frac{TP}{TP+FP}
%\end{equation}
%\begin{equation}
%	F\_score = \frac{2*Recall*Precision}{Recall+Precision}
%\end{equation}
where TP = True Positives; FP = False Positives; FN = False Negatives; TN = True Negatives.

%Recall measures how many of the vulnerable methods can be correctly detected, while Precision is used to measure how many of the detected vulnerable methods are indeed labeled as vulnerable in the collected dataset. F-score is used to reflect the overall performance by combining both the Recall and Precision.

%\noindent\textbf{RQ2. Comparison on Vulnerability Assessment on C dataset.}
\subsubsection{Comparison on Vulnerability Assessment on BigVul (RQ2).~\\}

{\em Baseline.} We compare {\tool} with DeepCVA~\cite{deepCVA-ase21}.


{\em Procedure.} We used BigVul dataset with the same
longitudinal setting in~\cite{deepCVA-ase21,falessi2020need} for
training, validation, and testing to mimic the real-world scenario in
which the older vulnerabilities are used for training to assess the
newer. Specifically, we sorted all the commits in a chronological
order based on their absolute time. We divided the commits into 10
equal folds from oldest to newest. For a fold $k$ ($k$ $\le$ $8$), we
used all the folds 1 to $k$ for training, the $(k$+$1)^{th}$ and
$(k$+$2)^{th}$ folds for validation and testing. Models are tuned as
in~RQ1.

%\subsubsection{\textbf{Evaluation Metrics}}

%To evaluate the models' performance in automated vulnerability
%assessment,

\emph{Evaluation Metrics.} We use the same metrics in
DeepCVA~\cite{deepCVA-ase21}: F-score and Matthews Correlation
Coefficient (MCC). F-score ranges from 0 to 1 (the best), to evaluate
classification tasks and to handle the class imbalance prevalent in
some of the VATs. MCC ranges from -1 to 1 (the best).
%and MCC ranges from -1 to 1 (the best). F-score is a metric to
%evaluate classification tasks. We chose F-score to handle the class
%imbalance prevalent in some of the VATs.
Since we evaluate the classification models with multiple classes,
we used the macro F-score~\cite{spanos2018multi} and the multi-class
version of MCC~\cite{gorodkin04}. The overall MCC is computed as the
average of the MCCs for all classification tasks as in
DeepCVA~\cite{deepCVA-ase21}.

%We turned {\tool} with the following key hyper-parameters ....
%We turned DeepCVA with the following key hyper-parameters ...
%We use macro F1-Score and multi-class MCC to evaluate both approaches.


%We also tuned the models as in RQ1.

%In this RQ, we compare {\tool} with the same baseline in RQ1, DeepCVA. We ran {\tool} and DeepCVA on the Java dataset, DeepCVA-dataset~\cite{} (CVAD).  We used the same time-series data splitting strategy as the one in RQ1 to train, validate, and test both models.


%we randomly split the BigVul into xx\% for training, xx\% for validation, and xx\% for testing.


%We turned {\tool} with the following key hyper-parameters .... As we reused the dataset from the DeepCVA, we directly tune DeepCVA with the parameters reported in their paper. Same as RQ1, we use macro F1-Score and multi-class MCC to evaluate both approaches.

%\noindent\textbf{RQ3. Code Change Embedding Analysis.}
\subsubsection{Code Change Embedding Analysis (RQ3).}

We aim to evaluate the impact of our novel graph-based, contextualized
code change embedding model on {\tool}'s ability in class separation.

%{\tool} in the classification for VATs.

%\noindent \textbf{RQ4. Program Dependencies.}

\subsubsection{Program Dependencies (RQ4).}

We employ Explainable AI (XAI) to demonstrate the utilization of
program dependencies by our VDA model in vulnerability detection and
assessment. An XAI model enables us to analyze the essential features
within the input code that influence the model's predictive
outcomes. If XAI highlights program dependencies within the input
code, we can subsequently investigate whether VDA appropriately
employs these program dependency-related features for accurate
predictions.

%We use Explainable AI (XAI) to show that {\tool} in fact leverages
%program dependencies in detection and assessment. XAI allows us to
%examine what features in the input code are crucial for a model to
%make a prediction. If XAI points to program dependencies in the input
%code, we could examine whether {\tool} uses the correct features in
%program dependencies or not.
%------------------------------


%\vspace{1pt}
%\noindent\textbf{RQ5. Overlapping Analysis.}  We counted the
%vulnerabilities that were correctly assessed by {\tool} but
%not by DeepCVA and vice versa, and the ones
%that were correctly classified by both.


%\vspace{1pt}
%\noindent\textbf{RQ5. Ablation Study on Multi-Task Learning and Context.}

\subsubsection{Ablation Study on Multi-Task Learning and Context (RQ5).~\\}

We evaluate the impacts of the following factors in {\tool}: (1)
multi-task learning, and (2) change context. We conducted an analysis
by removing each factor from {\tool} and made a comparison.

%d it with the xcomplete model.

%\vspace{3pt}
%\noindent\textbf{RQ6. Comparison on Assessment on CVAD (Java dataset).}

\subsubsection{Comparison on Assessment on CVAD Java dataset (RQ6).~\\}

{\em Baseline and Procedure.} We compared {\tool} with the baseline
DeepCVA~\cite{deepCVA-ase21}. We used the same procedure, tuning, and
longitudinal setting as in RQ1, but on the Java~dataset.

%For (3), we used GNNExplainer~\cite{GNNExplainer} (Section~\ref{discuss:sec}).



%We aim to show the insights that {\tool}'s embeddings for code changes
%help improve automated vulnerability assessment over
%DeepCVA~\cite{deepCVA-ase21}.



%We aim to study the projections of the code changes in the commits
%with respect to different vulnerability assessment types.

