\subsubsection{\bf Sensitivity Analysis (RQ6)}

\begin{table}[t]
	\caption{RQ6. Impact of Key Components}
	\vspace{-0.1in}
	\begin{center}
%		\scriptsize
\small
		\tabcolsep 4pt
		\renewcommand{\arraystretch}{1} \begin{tabular}{p{3.5cm}<{\centering}|p{2cm}<{\centering}p{1.2cm}<{\centering}}
			
			\hline
			                         & macro  F-Score & MCC \\ 
			\hline
			
			\tool w/o Context        &  0.61    & 0.31         \\
			\tool w/o Multi-task Learning  &  0.60    & 0.30         \\
			\tool                    &  0.64    & 0.33         \\
			\hline
		\end{tabular}
		\label{RQ4-result-1}
	\end{center}
\end{table}

As seen in Table~\ref{RQ4-result-1}, without~the context, the macro
F-score and multi-class MCC decrease by 4.7\% and 6.1\%,
respectively.
%in comparison with the complete model.
Without the multi-task learning, the macro F-score and multi-class
MCC decrease by 6.3\% and 9.1\%, respectively. While both the
context and multi-task learning play positive roles in 
{\tool}'s accuracy, multi-task learning contributes slightly
more.

%Table~\ref{RQ4-result-1} shows the overall macro-F-score and multi-class MCC results when the key features in {\tool} were removed. As seen in Table~\ref{RQ4-result-1}, without the context vector, the macro-F-score and multi-class MCC decrease by 4.7\% and 6.1\%, respectively. And without the multi-tasking framework, the macro-F-score and multi-class MCC decrease by 6.3\% and 9.1\%, respectively. While both context vector and the multi-tasking framework play a positive role in the overall performance, the multi-tasking framework contributes more to {\tool}.

\begin{table}[t]
	\caption{RQ6. Impact of Num. of Hops $k$ for Context Size}
	\vspace{-0.1in}
	\begin{center}
%		\scriptsize
\small
		\tabcolsep 4pt
		\renewcommand{\arraystretch}{1} \begin{tabular}{p{3.5cm}<{\centering}|p{2cm}<{\centering}p{1.2cm}<{\centering}}
			
			\hline
			& macro F-Score & MCC \\ 
			\hline
			\tool ($k=1$)          & 0.62 & 0.32          \\
			\tool ($k=2$)          & 0.63 & 0.33          \\
			\tool ($k=3$)          & 0.64 & 0.33          \\
			\tool ($k=4$)          & 0.62 & 0.31          \\
			\tool ($k=5$)          & 0.61 & 0.30          \\
			\hline
		\end{tabular}
		\label{RQ4-result-2}
	\end{center}
\end{table}

Table~\ref{RQ4-result-2} shows the impact of the context size $k$ (the
number of hops from a changed node). As seen, when
%the context size
$k$ increases from 1--3, the macro F-score increases to its highest
value of 0.64 and the multi-class MCC increases to its highest value
of 0.33. However, when $k$ continues to increase $k \geq 3$, both
macro F-score and multi-class MCC decrease. The rationale is that as
the context size is too small, the limited number of surrounding nodes
cannot capture well the relevant statements for {\tool} to perform
vulnerability assessment. As the context size gets larger, the
increasing number of the irrelevant statements will bring in
biases. Thus, we selected $k$=3 as the default context size for the
other studies on the C dataset.

%Table~\ref{RQ4-result-2} shows the overall macro-F-score and multi-class MCC when we evaluate the size $K$ of a context (i.e., the number of hops from the changed node) on the C dataset. From the table, we can see that when the number of hops from the changed node increase from $K=1$ to $K=3$, the macro-F-score increases to its highest value $0.64$ and the multi-class MCC increases to its highest value $0.33$. But after $K=3$, with the rise of $K$ value, both macro-F-score and multi-class MCC decrease. The reason that causes this is that when the $K$ value is too small, the limited amount of surrounding nodes cannot capture all the relevant nodes well for \tool to learn the context information well. But if the $K$ value is too big, the increasing number of the irrelevant nodes will bring in too many biases and hurt the overall performance of \tool. Therefore, we select $K=3$ to be the best setting for \tool on our C dataset with this analysis.  















\iffalse

\begin{table}[h]
	\caption{Sensitive Analysis -- Impact of Different Factors on {\tool}'s Accuracy in terms of Top1 on BigFix Dataset. 
	%	Seq2seq: Simple Sequence to Sequence model; Two-Layer-EDM: Two Layers Tree-Based LSTM Encoder-Decoder Model; PAT: Program Analysis Techniques including Renaming and Program Analysis Filters.
	}
	\vspace{-10pt}
	\begin{center}
		\renewcommand{\arraystretch}{1} 
		\begin{tabular}{l|p{0.7cm}<{\centering}|p{1.5cm}<{\centering}}
			\hline
			Models & Top1 & Improvement\\
			\hline
			Seq2Seq & 1.8\% & \\
			Seq2Seq + PAT & 6.4\% & 256\% \\ 
			Two-Layer-EDM & 11.7\% & 550\%\\
			Two-Layer-EDM + PAT &  24.4\% &109\%\\
			Two-Layer-EDM + PAT + Re-ranking & 29.4\%&20.5\% \\
			\hline
		\end{tabular}
	Seq2seq: a simple sequence-to-sequence model; Two-Layer-EDM: Two-layer tree-based LSTM encoder-decoder model; 
	PAT: program analysis (PA) Techniques including Renaming and PA Filters.
		\label{RQ3}
	\end{center}
%\vspace{-10pt}
\end{table}

%We conducted an experiment to study how different factors including renaming, two Layer encoder-decoder model, models for encoders and decoders, program analysis filters, re-selecting model affect our model's accuracy.

Table~\ref{RQ3} shows that we build three variants of {\tool} with
different factors and their combinations.  We analyze our results as
follows:

(1) \textbf{Impact of Two-Layer-EDM}. Our Two-Layer-EDM
can improve the one-layer sequence-to-sequence model by 550\% and
using only {\em seq2seq} cannot get good results. Two-layer-EDM is
designed to learn the local context of a bug fix and code
transformations.

(2) \textbf{Impact of PAT}. Using program analysis (PA) techniques,
PAT, including alpha-renaming and PA-filtering, is effective to
improve Two-Layer-EDM by 109\%.
%This is reasonable because the program analysis techniques can make our data become more common in order to make the training data can contribute more and also can help reduce the wrong candidate patch by program analysis filters to reduce the search space and increase the accuracy.
The alpha-renaming process can help improve {\tool} for better training
and the PA-filtering process can help elminate more the irrelevant patches.
%From \textit{Seq2seq} in Table~\ref{RQ3}, we can see that by only using one layer simple seq2seq model to do the bug fix cannot get a good result on BigFix.
%To study the impact of our two layers tree-based encoder decoder model, we compare the results obtained from two variants: \textit{Seq2seq} and \textit{TLTM}. The results show that using the our two layers tree-based encoder decoder model can increase accuracy relatively by 550\%. This is because TLTM can analysis local context and code transformation at the same time with less biases.

Adding program analysis to the basic {\em seq2seq} model can improve
it by 256\%. However, the Two-Layer-EDM can improve seq2seq by 550\%.
Thus, the Two-Layer-EDM has more impact than PAT.

(3) \textbf{Impact of Re-ranking}. The results
of \textit{Two-Layer-EDM + PAT + Re-Ranking} show that having
re-ranking can increase accuracy relatively by 20.5\%. The reason is
that the re-ranking process, which uses a Convolutional Layer to
distinguish the best result from the others, can help increase
{\tool}'s accuracy by pushing the right results to the top of the list
of the candidate fixes.

\fi

%The re-ranking part which uses convolutional layer to classify the
%best result and other results can help increase the accuracy of our
%approach. In this way, the re-ranking can help our approach a lot.
%The Re-ranking can improve the model with Two-layer-EDM + PAT by
%20.5\%. The results show that once the model generates a list of
%candidate patches, re-ranking the list can help improve the results.
