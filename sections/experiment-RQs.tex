%\subsection{Research Questions}
%To evaluate {\tool}, we answer the following questions:

%\vspace{3pt}

\vspace{1pt}
\noindent\textbf{RQ1. Comparison with State-of-the Art ML-based Vulnerability
Detection Approaches on C dataset.} How well does {\tool} perform
compared with the existing DL-based VD approaches?

\vspace{1pt}
\noindent\textbf{RQ2. Comparison in Vulnerability Assessment on a C
  Dataset.} How well does {\tool} perform compared to the
state-of-the-art model in vulnerability assessment on a C dataset?

  %Vulnerability Assessment Comparative Study on Java Dataset.} How well does {\tool} perform in comparison with
%the state-of-the-art approaches for assessing vulnerabilities in Java?

\noindent\textbf{RQ3. Contextualized Embeddings for Code Changes.} Do {\tool}'s embeddings help it improve over the
baseline in classifications?

%What are the characteristics of code change embeddings in VA?

\noindent {\bf RQ4. Explainable AI to Study Relevant Features on Program Dependencies.} Does {\tool} use program dependencies in vulnerability detection and assessment?

In RQ3 and RQ4, we aim to evaluate the extent of contributions of two
  {\tool}'s key design choices, i.e., {\em contextualized embeddings
  for code changes} and~{\em program dependencies} to its performance.

%\vspace{3pt}
%\noindent\textbf{RQ5. Overlapping Analysis.} How many vulnerabilities
%are assessed correctly by a model and not by another one?

%\vspace{3pt}
\noindent\textbf{RQ5. Ablation Study on Multi-Task Learning and Context.} How do multi-task
learning and context affect {\tool}'s performance in vulnerability
detection and assessment?

\vspace{1pt}
\noindent\textbf{RQ6. Comparison in Vulnerability Assessment on a Java
  Dataset.} How does {\tool} perform compared to the
state-of-the-art model in vulnerability assessment on a Java dataset?

\input{sections/datasets}
