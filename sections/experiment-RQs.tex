%\subsection{Research Questions}
%To evaluate {\tool}, we answer the following questions:

%\vspace{3pt}

\vspace{1pt}
\noindent\textbf{RQ1. Comparison with State-of-the Art ML-based Vulnerability
Detection (VD) Approaches on C dataset.} How well does {\tool} perform
compared with the existing DL-based VD approaches?

\vspace{1pt}
\noindent\textbf{RQ2. Comparison in Vulnerability Assessment on C
  Dataset.} How well does {\tool} perform compared to the
state-of-the-art model in vulnerability assessment on a C dataset?

  %Vulnerability Assessment Comparative Study on Java Dataset.} How well does {\tool} perform in comparison with
%the state-of-the-art approaches for assessing vulnerabilities in Java?

\noindent\textbf{RQ3. Contextualized Embeddings for Code Changes.} Do {\tool}'s embeddings help it improve over the
baseline in classifications?

%What are the characteristics of code change embeddings in VA?

\noindent {\bf RQ4. Program Dependencies.} Does {\tool}'s integration
of program dependencies help in vulnerability detection and
assessment?

In RQ3 and RQ4, we aim to evaluate the insights that the two~key
  designs of {\tool} in {\em contextualized embeddings for code
  changes} and~{\em program dependencies} contribute to its performance.

%\vspace{3pt}
%\noindent\textbf{RQ5. Overlapping Analysis.} How many vulnerabilities
%are assessed correctly by a model and not by another one?

%\vspace{3pt}
\noindent\textbf{RQ5. Ablation Study on Multi-Task Learning and Context.} How do multi-task
learning and context affect {\tool}'s performance in vulnerability
detection and assessment?

\vspace{1pt}
\noindent\textbf{RQ6. Comparison in Vulnerability Assessment on Java
  Dataset.} How does {\tool} perform compared to the
state-of-the-art model in vulnerability assessment on a Java dataset?
