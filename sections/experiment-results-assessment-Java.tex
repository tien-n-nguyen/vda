\subsection{\bf Comparative Study on Java Dataset (RQ6)}

\begin{table}[t]
	\caption{Vulnerability Assessment on Java Dataset (RQ6)}
        \vspace{-9pt}
	\begin{center}
        \tabcolsep 2.5pt
%		\footnotesize
\small
		\renewcommand{\arraystretch}{1}
		\begin{tabular}{l|p{2.0cm}<{\centering}|p{1.6cm}<{\centering}|p{1.5cm}<{\centering}}
			\hline
			\multirow{2}{*}{CVSS Metric}     & \multirow{2}{*}{Evaluation Metric}  & \multicolumn{2}{c}{Model}\\
			\cline{3-4}
			&                                     & DeepCVA~\cite{deepCVA-ase21}    & \tool       \\
			\hline
			\multirow{2}{*}{Confidentiality} & macro F-score                             &     0.44       & 0.55\\
			\cline{2-4}
			& MCC                                 &      0.27      & 0.32\\
			\hline
			\multirow{2}{*}{Integrity}       & macro F-score                             &    0.43        & 0.52\\
			\cline{2-4}
			& MCC                                 &    0.25        & 0.27\\
			\hline
			\multirow{2}{*}{Availability}    & macro F-score                             &   0.43         & 0.54\\
			\cline{2-4}
			& MCC                                 &    0.27        & 0.27\\
			\hline
			\multirow{2}{*}{Access Vector}   & macro F-score                             &   0.55         & 0.59\\
			\cline{2-4}
			& MCC                                 &    0.13        & 0.17\\
			\hline
			\multirow{2}{*}{Access Complexity} & macro F-score                           &   0.46         & 0.53\\
			\cline{2-4}
			& MCC                                 &    0.24        & 0.26\\
			\hline
			\multirow{2}{*}{Authentication}  & macro F-score                             &   0.66         & 0.68\\
			\cline{2-4}
			& MCC                                 &   0.35         & 0.38\\
			\hline
			\multirow{2}{*}{Severity}        & macro F-score                             &   0.42         & 0.51\\
			\cline{2-4}
			& MCC                                 &   0.21         & 0.22\\
			\hline
			\hline
			\multirow{2}{*}{Average}         & macro F-score                             &    0.45        & 0.59 ($\Uparrow${\bf 31.0\%})\\
			\cline{2-4}
			& MCC                                 & 0.24           & 0.32 ($\Uparrow${\bf 33.3\%})\\
			\hline
                        \hline
\multirow{2}{*}{Vulnerability Detection} &  & VCCFinder & CAT \\
\cline{2-4}
 & F-score & 0.24 & 0.76 \\
\hline
		\end{tabular}
		\label{rq2_results}
	\end{center}
\end{table}

We aim to show that our approach also works for vulnerabilities in a
different programming language. Table~\ref{rq2_results} shows that it relatively
improves DeepCVA~\cite{deepCVA-ase21} by {\em 31.0\% in macro F-score
and 33.3\% in multi-class MCC} on the overall multi-class
classification in Java dataset. For specific VATs, {\tool} improves
DeepCVA by {\em 3.0--25.6\% in macro-F-score and 30.8\% in
multi-class MCC}. Moreover, the largest relative improvement in macro
F-score happens for {\em Availability} and the largest one in
multi-class MCC happens for {\em Access Vector}.
%The lowest relative improvement in both macro F-score and
%multi-class MCC happens for {\em Authentication}, however,
The absolute macro F-score value (0.68) for {\em Authentication} is
highest among all the VATs.
%
The F-score for detection from {\tool} is also higher than that of the
commit-level VCCFinder, which uses SVM. Other baselines in
Table~\ref{tab:rq1} do not work on Java. In brief, this result is
consistent with the trend as {\tool} being run on the C dataset.


%Thus, {\tool}'s assessment works for Java.

%Table~\ref{rq2_results} shows that {\tool} outperforms the state-of-the-art baseline, DeepCVA, in the automation of commit-level vulnerability assessment on the Java dataset. Particularly, {\tool} improves the state-of-the-art baseline by 14.3\% in terms of macro-F-score, and 8.0\% in terms of multi-class MCC on the overall performance. The higher values of macro-F-score and MCC indicate that {\tool} can perform more accurate multi-classification on Java commits than DeepCVA. As for specific types of vulnerability assessment, {\tool} improves the state-of-the-art baseline by 3.0-25.6\% in terms of macro-F-score and 0-30.8\% in terms of macro-F-score. The lowest relative improvement of both macro-F-score happens when testing the $Authentication$. And when testing $Availability$, \tool and DeepCVA get the same multi-class MCC. As for the biggest relative improvement, when testing $Availability$, \tool achieves the biggest relative improvement on the macro-F-score, and the biggest relative improvement of multi-class MCC happens when testing $Access Vector$.

























